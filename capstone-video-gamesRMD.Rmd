---
title: "Capstone - Predicting Video Game Sales"
author: "Lionel Fournier"
date: "14/06/2020"
output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\pagebreak

# Overview
In a few decades, video games have reached a major position in the entertainement industry. If there are still small independent games made by one person, today standard is the triple A, that cost huge amount of money and need big team to be made. It's important to have an idea of how you can optimise sales before taking such a big risk. Data science can help you with that.

# Introduction
In this project, we are going to use machine learning to predict the ratings users. We are going to use a dataset containing a list of video games with sales greater than 100,000 copies. It was generated by a scrape of vgchartz.com. This data set contains video games released before 2017.
We are going to first prepare the data and then analyse it. We then will construct our machine algorithm, starting trying different method to achieve better result.

# Methodology
First we are going to download and prepare the data. Then we are going to analyse them. The goal is to dive in some element that may affect our different models. 

We are specificaly going to look into global sales and how the different variables impact it. Different gaming systems, number of critics, the score of the games with critics and users can all affect global sales. Different genres may also have an influence on the global sales.

To take into account critic and user scores, we are going to have to limit the dataset. Unfortunately, there are a lot of missing values for those variables, and we want to be able to use them. Although, the set will remain large enough with more almost 7000 entries.

From there, we are going to create our models, taking into account what we learned in our analysis. First we are going to split it into a testing set and a training one. Then we are going to look at which predictors are the most relevant and then we are going to use different machine learning techniques such as linear model, generelized linear model and random forest.

To test the accuracy of each model, we are going to use the residual mean squared error (RMSE). The lower the value will be the better.

\pagebreak


# Data preparation

## Creation of the data set

```{r datacreation, echo=TRUE, message=FALSE, warning=FALSE, eval=TRUE}

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(cowplot)) install.packages("RCurl", repos = "http://cran.us.r-project.org")
if(!require(olsrr)) install.packages("RCurl", repos = "http://cran.us.r-project.org")
if(!require(RCurl)) install.packages("RCurl", repos = "http://cran.us.r-project.org")

#This script was made on R 3.6.2
#Download the data from my github repo 
dl <-getURL("https://raw.githubusercontent.com/lifnr/video_games_machine_learning/master/Video_Games_Sales_as_at_22_Dec_2016.csv")
Video_Games<-read.csv(text = dl)
vg<-Video_Games
```

## Data preparation

Our first step will be to have a quick look at the data. Then make some minor changes for ease of use. As mentioned, one problem we face is missing users and critics data. For now, we are going to keep everything. We are going to remove all the rows with NA later in the process to be able to use the full extent of the data.
```{r checkData, echo=TRUE, message=FALSE, warning=FALSE, eval=TRUE}
#Checking the first entries of the edx dataset
head(vg)
#Checking the structure of the data
str(vg)
#Checking the dimension of the data
dim(vg)
#Number of NA in the dataset
sum(is.na(vg))

#Changing the year into integers and user score as numeric
vg<-vg%>%mutate(Year_of_Release=as.integer(as.character(Year_of_Release)), User_Score=as.numeric(as.character(User_Score)))
```


We also want to create a new column with the gaming system. The plateform is mentionned for every game, but we can gather that in 4 big family of gaming systems (Nintendo, Sony, Xbox and PC) and older ones like Sega and others. 

```{r system, echo=TRUE, message=FALSE, warning=FALSE, eval=TRUE}

nes <- c("3DS","DS","GB","GBA","N64","GC", "NES","SNES","Wii","WiiU")
sony <- c("PS","PS2","PSP","PS3","PS4","PSV")
sega <- c("GEN","SCD","DC","GG","SAT")
xbox <- c("XB","X360", "XOne")
other <- c("2600","3DO","NG","PCFX","TG16","WS")
pc <- c("PC")

vg$System[vg$Platform %in% nes] <- "Nintendo"
vg$System[vg$Platform %in% sony] <- "Sony"
vg$System[vg$Platform %in% xbox] <- "XBox"
vg$System[vg$Platform %in% sega] <- "Sega"
vg$System[vg$Platform %in% pc] <- "PC"
vg$System[vg$Platform %in% other] <- "Other"
```

\pagebreak
# Analysis

Before we start building our models, we need to analyse our data in order to better understand the importance of different predictors. 

## Sales

```{r salesYearandGame, echo=FALSE} 
#Plot of sales per year
slperYear<-vg%>%filter(Year_of_Release<=2016)%>%
  group_by(Year_of_Release)%>%
  summarize(Sales= sum(Global_Sales))%>%
  ggplot(aes(Year_of_Release, Sales))+
  geom_col()

#Plot of number of games per year
gameperYear<-vg%>%filter(Year_of_Release<=2016)%>%
  group_by(Year_of_Release)%>%
  summarize(Games_per_Year=n())%>%
  ggplot(aes(Year_of_Release, Games_per_Year))+
  geom_col()

#the two plots togheter
plot_grid(slperYear, gameperYear, labels = "AUTO")
``` 
We can see that sales have been growing since the 1980's. They reached a peak around 2010. The number of games and the sales have droped after that. We can make the hypothesis that the financial crisis of 2008 had an impact on many households which may have limited the number of games bought by many people. 
As a side note, when the data will be available, it would be interesting to see how the coronavirus pandemic has affected the video game sales. The lockdown has given an incentive for people to find an occupation at home. Such as, playing video games, and at the moment of writing this report, there is a shortage of Nintendo Switch.


```{r salesPerRegion, echo=FALSE} 
#Sales per region
#Plot of sales EU per year
pl_EU<-vg%>%filter(Year_of_Release<=2016)%>%
  group_by(Year_of_Release)%>%
  summarize(Sales= sum(EU_Sales))%>%
  ggplot(aes(Year_of_Release, Sales))+
  geom_col()+
  ylab("Sales EU")

#Plot of sales Jap per year
pl_Jap<-vg%>%filter(Year_of_Release<=2016)%>%
  group_by(Year_of_Release)%>%
  summarize(Sales= sum(JP_Sales))%>%
  ggplot(aes(Year_of_Release, Sales))+
  geom_col()+
  ylab("Sales Japan")

#Plot of sales NA per year
pl_NA<-vg%>%filter(Year_of_Release<=2016)%>%
  group_by(Year_of_Release)%>%
  summarize(Sales= sum(NA_Sales))%>%
  ggplot(aes(Year_of_Release, Sales))+
  geom_col()+
  ylab("Sales NA")

#Plot of sales OTher per year
pl_Ot<-vg%>%filter(Year_of_Release<=2016)%>%
  group_by(Year_of_Release)%>%
  summarize(Sales= sum(Other_Sales))%>%
  ggplot(aes(Year_of_Release, Sales))+
  geom_col()+
  ylab("Sales Other")

#The four region together
plot_grid(pl_EU, pl_NA, pl_Jap,pl_Ot, labels = "AUTO")
```

The down trend after 2010 is noticeable in the four sales regions included in the data. Altough it's interesting to notice Japan particularity. Video games seem to be way more largely sold there, and it's logical with some of the main video game companies based in Japan. This country also had a less steep curve after 2010, losing sales more gradualy.

This market, while very important, is smaller than the European one. But the main one remains the North America one, with more sales than all the others combined at the peak of the curve. It would be wise to take that market in consideration when making a video game.


## Genre and plateform
Diffrent aspects are going to influence the number of sales. Some genres are more popular, while others aim at a more niche audience. The plateform also has an influence. Realising a game only in one system may limit the number of units sold, especially if the plateform hasn't met a huge success.

```{r perGenre, echo=FALSE} 
#Plot of sales per genre
vg%>%
  group_by(Genre)%>%
  summarize(Sales= sum(Global_Sales))%>%
  ggplot(aes(x=reorder(Genre, -Sales), Sales))+
  geom_col()+
  xlab("Genre")+
  theme(axis.text.x=element_text(angle=90,hjust=1)) 

```
We can see that the most sales are made for games within  action, sports and shooters genres. On the other side, strategy or adventure games have lower sale numbers. Those games do have a strong community, but require more engagement, and seem to be more of a niche market.

```{r platform, echo=FALSE} 
#Plot of sales per plateform
vg%>%
  group_by(Platform)%>%
  summarize(Sales= sum(Global_Sales))%>%
  ggplot(aes(x=reorder(Platform, -Sales), Sales))+
  geom_col()+
  theme(axis.text.x=element_text(angle=90,hjust=1)) 

vg%>% group_by(System)%>% 
  summarise(Sales = sum(Global_Sales))%>%
  ggplot(aes(x=reorder(System, -Sales), Sales))+
  geom_bar(stat = "identity")+
  xlab("Gaming system")

```
Plateform matters, not for a question of console wars, but for sales. We can see that Sony and Nintendo game consoles are two platforms where you can sell more game. It's also interesting to see that PC is kind of far away in term of sales. It does require a larger investment to buy a gaming pc, while a gaming consoles may be a fraction of the price. Although, to maximize sales, a lot of editors choose to release their games on almost every current platform. 

```{r Publisher, echo=FALSE} 
#Publisher with most sales
vg %>% group_by(Publisher) %>%
  summarize(count = sum(Global_Sales)) %>%
  arrange(desc(count))

#Games with most sales
vg %>% group_by(Name) %>%
  summarize(count = sum(Global_Sales)) %>%
  arrange(desc(count))

```
It may be smart to be on multiple consoles, although the most successful editor in term of sales is Nintendo, with exclusivity for its own gaming system. Six of the ten most sold games are from Nintendo. 

## User and critic effect

Now to analyse the impact of user and critic effect, we are going to remove all the rows with NA from the data set. This limited dataset will also be used for our modelling.

```{r removingNA, echo=TRUE, message=FALSE, warning=FALSE, eval=TRUE}
#Removing NA from the set and keeping only variable we are going to use
vg_short <- vg%>%filter(vg$Year_of_Release<=2016)%>%
  select(Global_Sales,Name,Year_of_Release,Genre, Critic_Score, Critic_Count,User_Score,User_Count,System)%>%
  na.omit(vg) 
sum(is.na(vg_short))
dim(vg_short)
```
As we can see there are no more NA's, and the dataset is now 6894 rows long. While doing this step we also selected the column we are going to use in our model and removed the other ones.


```{r rating, echo=FALSE, fig.align='center'} 
#we look at the impact of critic rating
cs<-vg_short%>%
  group_by(Critic_Score)%>%
  summarize(Sales= sum(Global_Sales))%>%
  ggplot(aes(Critic_Score, Sales))+
  geom_col()+
  scale_x_continuous()

#Sales vs critic_count
cc<-vg_short%>%
  group_by(Critic_Count)%>%
  summarize(Sales= sum(Global_Sales))%>%
  ggplot(aes(Critic_Count, Sales))+
  geom_col()+
  scale_x_continuous()

plot_grid(cs, cc, labels = "AUTO")

#user count impact on sales
us<-vg_short%>%
  group_by(User_Score)%>%
  summarize(Sales= sum(Global_Sales))%>%
  ggplot(aes(User_Score, Sales))+
  geom_col()


#Sales vs user_count
uc<-vg_short%>%
  group_by(User_Count)%>%
  filter(User_Count<200)%>%
  summarize(Sales= sum(Global_Sales))%>%
  ggplot(aes(User_Count, Sales))+
  geom_col()+
  scale_x_continuous()

plot_grid(us, uc, labels = "AUTO")
```


# Modeling
Based on our analysis, we are going to build few machine learning algorithms through a training set and a test set. Our goal is to achieve the lowest RMSE. We did most of the data preparation previously.

```{r creatingTrainTestSet, echo=TRUE, message=FALSE, warning=FALSE, eval=TRUE}
#Creating a test_set and a training set

set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = vg_short$Global_Sales, times = 1,
                                  p = 0.2, list = FALSE)
train_set <- vg_short[-test_index,]
test_set <- vg_short[test_index,]

```

## Subset

Our first step will be to look which predictors may be useful for our data.

```{r subset, echo=FALSE,  warning=FALSE,  message=FALSE} 
#Subset selection
mod <- lm(Global_Sales~Year_of_Release+Genre+Critic_Score+ Critic_Count+User_Score+User_Count+System, data=train_set)
pred<-ols_step_best_subset(mod)
pred

```

We can see here that critic and user count may be our first predictors. That does make sense, as if they are more critics available, more people will be able to learn about the game and then buy it. However, our analysis was a bit sceptical about that effect, we are going to use it and add other variables in our model.


## Linear model

Our first two models are going to use a linear model. At firest, we will only use user and critic count and then add the scores and system.

```{r firstModel, echo=TRUE, warning=FALSE} 
fit <- train(Global_Sales~Critic_Count+User_Count, data=train_set, method="lm")
trn <- predict(fit,train_set)
RMSE(trn, test_set$Global_Sales)
fit$results['RMSE']


fit2 <- train(Global_Sales~Critic_Score+Critic_Count+User_Score+User_Count+System, data=train_set, method="lm")
trn2<- predict(fit2,train_set)
RMSE(trn2, test_set$Global_Sales)
fit2$results['RMSE']
```
The first model gives us a rmse of 1.900543. The second 1.890291 showing an improvement.


## Generalized Linear Model

Now we will use Generalized Linear Model, with the same values from our second one with lm.
```{r secondModel, echo=TRUE, warning=FALSE}
#GLM Model
glm_fit <- train(Global_Sales~Critic_Score+Critic_Count+User_Score+User_Count+System,
             data=train_set, method="glm")
trn_glm <- predict(glm_fit,train_set)
RMSE(trn_glm, test_set$Global_Sales) 
glm_fit$results['RMSE']

```

With a RMSE of 1.800426, this model shows clear improvement, but leaves some room to do better.


## Random forest

Finally, we are going to use the Random forest model.

```{r thirdModel, echo=TRUE, warning=FALSE} 
#Random forest model
rf_fit <- train(Global_Sales~Critic_Score+Critic_Count+User_Score+User_Count+System,
                 data=train_set, tuneLength=2, method="rf")
trn_rf <- predict(rf_fit,train_set)
RMSE(trn_rf, test_set$Global_Sales) 
rf_fit$results[row.names(rf_fit$bestTune),'RMSE']

```

With a new rmse of 1.582424, we definitely improve our prediction using the random forest.
\pagebreak

# Results

As the result of our different model, we can see a clear improvement from our first model with a rmse of 1.900543 to the final one with a rmse of 1.582424. Adding predictors helped us reduce the rmse, but the main diffrence is the use of the random forest.

# Conclusion

Our models help us predict video game sales and which variables to look for. It may be helpful when developing the next big it, but major companies already have figured it out in a lot of aspects. Major triple A are supported by a very consequent marketing budget, ensuring maximum visibility, and are on multiplatforms. Although Nintendo shows that you can be strong with only one plateform. It should be possible to improve the model further by adding other effects that may influence global sales.
It will also be possible to add tuning parameters for each of the effects, in order to better fine tune the model. We may also use different machine learning models, as matrix factorization to improve the rmse, although it may be heavy on computers that don't have the necessary hardware.


